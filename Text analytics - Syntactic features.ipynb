{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from itertools import groupby\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.tokenize import PunktSentenceTokenizer, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "\n",
    "**pos_list** is the list of tags used by the nltk pos tagger.\n",
    "\n",
    "\n",
    "**X_tags** are lists of the different tags used by the stanford parser. See https://gist.github.com/nlothian/9240750.\n",
    "\n",
    "\n",
    "**path** should point to the folder where the stanford-parser/model files were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_list = '$ \\'\\' ( ) , -- . : CC CD DT EX FW IN JJ JJR JJS MD NN NNP NNPS NNS PDT POS PRP PRP$ RB RBR RBS RP SYM TO UH VB VBD VBG VBN VBP VBZ WDT WP WP$ WRB `` LS'.split()\n",
    "\n",
    "word_tags = 'CC CD DT EX FW IN JJ JJR JJS LS MD NN NNS NNP NNPS PDT POS PRP PRP$ RB RBR RBS RP SYM TO UH VB VBD VBG VBN VBP VBZ WDT WP WP$ WRB # -LRB- -RRB- -None-'.split()\n",
    "phrase_tags = 'ADJP ADVP CONJP FRAG INTJ LST NAC NP NX PP PRN PRT QP RRC UCP VP WHADJP WHAVP WHNP WHPP X WHADVP'.split()\n",
    "clause_tags = 'S SBAR SBARQ SINV SQ'.split()\n",
    "punc_tags = '$ \\'\\' ( ) , -- . : ``'.split()\n",
    "\n",
    "all_tags = word_tags + phrase_tags + clause_tags + punc_tags\n",
    "list_of_tag_cats = [word_tags, phrase_tags, clause_tags, punc_tags]\n",
    "\n",
    "path = ''\n",
    "sp = StanfordParser(path_to_jar=path + '\\\\stanford-parser.jar',\n",
    "                    path_to_models_jar=path + '\\\\stanford-parser-3.9.1-models.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bbc_articles = []\n",
    "path = 'Articles/Articles/bbc'\n",
    "for file in os.listdir(path):\n",
    "    with open(path + '\\\\' + file, 'r', encoding='UTF-8') as f:\n",
    "        txt = f.read()\n",
    "    txt = txt.replace('\\n\\n', '. ')\n",
    "    article_sents = [sent for sent in sent_tokenize(txt)]\n",
    "    bbc_articles.append((article_sents, 'bbc'))\n",
    "        \n",
    "guard_articles = []\n",
    "path = 'Articles/Articles/Guardian'\n",
    "for file in os.listdir(path):\n",
    "    with open(path + '\\\\' + file, 'r', encoding='UTF-8') as f:\n",
    "        txt = f.read()\n",
    "    txt = txt.replace('\\n\\n', '').replace('\\n', '. ')\n",
    "    article_sents = [sent for sent in sent_tokenize(txt)]\n",
    "    guard_articles.append((article_sents, 'guard'))\n",
    "    \n",
    "mirror_articles = []\n",
    "path = 'Articles/Articles/Mirror'\n",
    "for file in os.listdir(path):\n",
    "    with open(path + '\\\\' + file, 'r', encoding='UTF-8') as f:\n",
    "        txt = f.read()\n",
    "    txt = txt.replace('\\n\\n', ' ').replace('\\n', '')\n",
    "    article_sents = [sent for sent in sent_tokenize(txt)]\n",
    "    mirror_articles.append((article_sents, 'mirror'))\n",
    "\n",
    "telegraph_articles = []\n",
    "path = 'Articles/Articles/Telegraph'\n",
    "for file in os.listdir(path):\n",
    "    with open(path + '\\\\' + file, 'r', encoding='UTF-8') as f:\n",
    "        txt = f.read().replace('\\n\\n', ' ')\n",
    "    article_sents = [sent for sent in sent_tokenize(txt)]\n",
    "    telegraph_articles.append((article_sents, 'telegraph'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### labour ####\n",
    "with open('Manifestos/Manifestos/LabourMani.csv', 'r', encoding='UTF-8') as f:\n",
    "    txt = f.read()\n",
    "lab_sents = [sent.replace('\\n', '') for sent in sent_tokenize(txt)]\n",
    "\n",
    "#### Cons ####\n",
    "with open('Manifestos/Manifestos/ConservMani.csv', 'r', encoding='UTF-8') as f:\n",
    "    txt = f.read()\n",
    "con_sents = [sent.replace('\\n', ' ') for sent in sent_tokenize(txt)]\n",
    "\n",
    "#### LibDem ####\n",
    "with open('Manifestos/Manifestos/LibDemMani.csv', 'r', encoding='UTF-8') as f:\n",
    "    txt = f.read()\n",
    "lib_sents = [sent.replace('\\n', '') for sent in sent_tokenize(txt)]\n",
    "\n",
    "#### SNP ####\n",
    "with open('Manifestos/Manifestos/SNPMani.csv', 'r', encoding='UTF-8') as f:\n",
    "    txt = f.read()\n",
    "snp_sents = [sent.replace('\\n', '') for sent in sent_tokenize(txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### labour ####\n",
    "with open('Manifestos/2010/LabourMani.txt', 'r', encoding='UTF-8') as f:\n",
    "    txt = f.read()\n",
    "lab_2010_sents = [sent.replace('\\n', '') for sent in sent_tokenize(txt)]\n",
    "\n",
    "#### Cons ####\n",
    "with open('Manifestos/2010/ConservMani.txt', 'r', encoding='UTF-8') as f:\n",
    "    txt = f.read()\n",
    "con_2010_sents = [sent.replace('\\n', ' ') for sent in sent_tokenize(txt)]\n",
    "\n",
    "#### LibDem ####\n",
    "with open('Manifestos/2010/LibDemMani.txt', 'r', encoding='UTF-8') as f:\n",
    "    txt = f.read()\n",
    "lib_2010_sents = [sent.replace('\\n', '') for sent in sent_tokenize(txt)]\n",
    "\n",
    "#### SNP ####\n",
    "with open('Manifestos/2010/SNPMani.txt', 'r', encoding='UTF-8') as f:\n",
    "    txt = f.read()\n",
    "snp_2010_sents = [sent.replace('\\n', '') for sent in sent_tokenize(txt)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_common_word_list(text, size=100):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update([',', '‘', '(', ')', '.', '--', ':', '[', ']', '\\'\\'', '``', '-', ',', '’', '–', ';'])\n",
    "    count = Counter([word.lower() for block, _ in text \n",
    "                                  for sent in block for word in nltk.word_tokenize(sent) \n",
    "                                  if word.lower() not in stop])\n",
    "    return count.most_common(size)\n",
    "\n",
    "def get_max_width(node):\n",
    "    stack = [node]\n",
    "    max_width = 0\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        if isinstance(node, str):\n",
    "            continue\n",
    "        max_width = max(max_width, len(node))\n",
    "        for child in node:\n",
    "            stack.append(child)\n",
    "    return max_width\n",
    "\n",
    "def create_blocks(texts, chunk_size=10):\n",
    "    blocks = []\n",
    "    for sents, label in texts:\n",
    "        blocks += [(sents[i:i+chunk_size], label) for i in range(0, len(sents), chunk_size)]\n",
    "    return blocks\n",
    "\n",
    "def calc_vocab_score(word_counts):\n",
    "    m1 = len(word_counts)\n",
    "    m2 = sum([len(list(g))*(freq**2) for freq, g in groupby(sorted(word_counts.values()))])\n",
    "    if m2 == m1:\n",
    "        return 0\n",
    "    return (m1*m1)/(m2-m1)\n",
    "\n",
    "def normalize_counts(array):\n",
    "    if array.sum():\n",
    "        array = array / array.sum()\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis functions\n",
    "\n",
    "Functions for creating features sets for blocks of texts\n",
    "\n",
    "**full_analysis:** records all phrasal and word tags produced by Stanford parser as well as tree height and max width. Normalises each category of tag values and averages tree heights/widths for the given block of text. **slow**\n",
    "\n",
    "**simple analysis:** records pos for given block of text using nltk pos tagger. Normalises values.\n",
    "\n",
    "**word_analysis:** records how frequently popular words are used, also gets average word and sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_analysis(text):\n",
    "    try:\n",
    "        parses = [parse for parses in sp.raw_parse_sents(text) for parse in parses]\n",
    "    except:\n",
    "        return np.array([])\n",
    "    \n",
    "    avg_parse_height = sum([parse.height() for parse in parses]) / len(parses)\n",
    "    avg_parse_width = sum(map(get_max_width, parses)) / len(parses)\n",
    "    pos_count = Counter([prod.lhs().__str__() for parse in parses for prod in parse.productions()[1:]])\n",
    "        \n",
    "\n",
    "    features = np.array([avg_parse_height, avg_parse_width], dtype='float64')\n",
    "    for tag_cat in list_of_tag_cats:\n",
    "        tag_counts = [pos_count[tag] for tag in tag_cat]\n",
    "        features = np.append(features, normalize_counts(np.array(tag_counts, dtype='float64')))\n",
    "    return features\n",
    "\n",
    "def word_analysis(text):\n",
    "    words = [word.lower() for sent in text for word in nltk.word_tokenize(sent)]\n",
    "    avg_word_length = sum(map(len, words)) / len(words)\n",
    "    avg_sent_length = sum(map(len, text)) / len(text)\n",
    "    word_counter = Counter(words)\n",
    "    word_counts = [word_counter[word] for word in word_list]\n",
    "    vocab_score = calc_vocab_score(word_counter)\n",
    "    \n",
    "    features = [[avg_word_length, avg_sent_length, vocab_score], normalize_counts(np.array(word_counts))]\n",
    "    return np.array([el for feature in features for el in feature], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(blocks, analysis_func):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i, (text, label) in enumerate(blocks):\n",
    "        if not i % int(len(blocks) / 10):\n",
    "            print('Blocks completed: {0} / {1}'.format(i, len(blocks)))\n",
    "        feats = analysis_func(text)\n",
    "        if feats.size == 0:\n",
    "            continue\n",
    "        features.append(feats)\n",
    "        labels.append(label)\n",
    "\n",
    "    return features, labels  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build datasets\n",
    "\n",
    "I used state of the union addresses since they come built in with nltk and are a good proxy while we don't have our dataset yet. All files take the format year-president.txt as far back as 1945-Truman.txt. Sampling the blocks of texts sppeds up the process since the Stanford parser is pretty slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = create_blocks([(lab_sents, 'lab'), (con_sents, 'con'), (lib_sents, 'lib'), (snp_sents, 'snp')])\n",
    "training_set, test_set = train_test_split(dataset, test_size=0.2)\n",
    "analysis = word_analysis\n",
    "\n",
    "word_list = create_common_word_list(training_set)\n",
    "\n",
    "train_data, train_labels = create_dataset(training_set, analysis)\n",
    "test_data, test_labels = create_dataset(test_set, analysis)\n",
    "\n",
    "bbc_data, _ = create_dataset(bbc_articles, analysis)\n",
    "guard_data, _ = create_dataset(guard_articles, analysis)\n",
    "mirror_data, _ = create_dataset(mirror_articles, analysis)\n",
    "telegraph_data, _ = create_dataset(telegraph_articles, analysis)\n",
    "lab_2010_data, _ = create_dataset(random.sample(create_blocks([(lab_2010_sents, 'lab')]), 10), analysis)\n",
    "con_2010_data, _ = create_dataset(random.sample(create_blocks([(con_2010_sents, 'con')]), 10), analysis)\n",
    "lib_2010_data, _ = create_dataset(random.sample(create_blocks([(lib_2010_sents, 'lib')]), 10), analysis)\n",
    "snp_2010_data, _ = create_dataset(random.sample(create_blocks([(snp_2010_sents, 'snp')]), 10), analysis) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = LinearSVC()\n",
    "mnb = MultinomialNB()\n",
    "gnb = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sgd = SGDClassifier()\n",
    "rfc = RandomForestClassifier() \n",
    "abc = AdaBoostClassifier()\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "clfs = [svc, mnb, gnb, lr, sgd, rfc, abc, mlp]\n",
    "clf_scores = []\n",
    "for clf in clfs:\n",
    "    clf.fit(train_data, train_labels)\n",
    "    print('{0} \\ntraining score: {1} \\ntest     score: {2}\\n\\n'.format(type(clf).__name__, \n",
    "                                                               clf.score(train_data, train_labels),\n",
    "                                                               clf.score(test_data, test_labels)))\n",
    "    clf_scores.append([type(clf).__name__, clf.score(train_data, train_labels), clf.score(test_data, test_labels)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
